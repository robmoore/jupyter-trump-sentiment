{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages JohnSnowLabs:spark-nlp:1.2.3 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = spark.read.option(\"header\", \"true\").csv(\"trumptweet-mod.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(X.1='1', X='4', text='RT @GOPBlackChick: Illegals must be deported, said @realDonaldTrump Glad somebody has the guts to use the D-word! https://t.co/y15YuRIE59', retweet_count='26', favorited='FALSE', truncated='FALSE', id_str='6.33E+017', in_reply_to_screen_name='NA', source='\"<a href=\"\"http://www.tweetcaster.com\"\" rel=\"\"nofollow\"\">TweetCaster for Android</a>\"', retweeted='FALSE', created_at='Mon Aug 17 12:22:27 +0000 2015', in_reply_to_status_id_str='NA', in_reply_to_user_id_str='NA', lang='en', listed_count='46', verified='FALSE', location='Gotham City', user_id_str='191986903', description='Do I look like Batman to you?', geo_enabled='FALSE', user_created_at='Fri Sep 17 21:55:51 +0000 2010', statuses_count='138514', followers_count='881', favourites_count='155', protected='FALSE', user_url='NA', name=' Red Hood ', time_zone='Eastern Time (US & Canada)', user_lang='en', utc_offset='-14400', friends_count='927', screen_name='Blaze_in_3D', country_code='NA', country='NA', place_type='NA', full_name='NA', place_name='NA', place_id='NA', place_lat='NA', place_lon='NA', lat='NA', lon='NA', expanded_url='https://www.yahoo.com/politics/trump-deport-criminal-aliens-126838527651.html', url='https://t.co/y15YuRIE59', Class='1')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data.head()\n",
    "\n",
    "# retweeted='FALSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break up into separate parts\n",
    "train_data, test_data, validation_data = labeled_data.randomSplit([0.6, 0.2, 0.2], seed=71082)\n",
    "\n",
    "# Note: This wouldn't work in a cluster\n",
    "def write_df(df, name):\n",
    "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "    Configuration = sc._gateway.jvm.org.apache.hadoop.conf.Configuration\n",
    "\n",
    "    tmp_name = name + \".tmp\"\n",
    "    df.coalesce(1).write.mode('overwrite').text(tmp_name)\n",
    "    \n",
    "    fs = FileSystem.get(Configuration())\n",
    "    # Assume one file output\n",
    "    file = fs.globStatus(Path(tmp_name + \"/*.txt\"))[0].getPath();\n",
    "    fs.rename(file, Path(name));\n",
    "    fs.delete(Path(tmp_name), True);\n",
    "\n",
    "# split training data into positive/negative\n",
    "positive_data = train_data.filter(train_data.Class == \"1\").select(\"text\")\n",
    "write_df(positive_data, \"trumptweet-pos.txt\")\n",
    "\n",
    "negative_data = labeled_data.filter(train_data.Class == \"0\").select(\"text\")\n",
    "write_df(negative_data, \"trumptweet-neg.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the input data to be annotated\n",
    "data = spark.read.json(\"Trump_2017-10-30.json.gz\")\n",
    "data = data.filter(data.lang == \"en\")\n",
    "# Drop RTs\n",
    "data = data.filter(data.retweeted_status.isNull())\n",
    "#data.cache()\n",
    "#data.count()\n",
    "#data.show()\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize tweets\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import trim\n",
    "\n",
    "import emot\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "user_regex = r\"@\\S+\"\n",
    "url_regex = r\"http[s]?://\\S+\"\n",
    "hashtag_regex = r\"#\\S+\"\n",
    "space_regex = r\"\\s{2,}|\\n\"\n",
    "\n",
    "def strip_emo(text):\n",
    "    for data in emot.emoji(text):\n",
    "        text = text.replace(data['value'], '')   \n",
    "    for data in emot.emoticons(text):\n",
    "        text = text.replace(data['value'], '')\n",
    "    return text\n",
    "\n",
    "strip_emo_udf = udf(strip_emo, StringType())\n",
    "\n",
    "# Remove users (@foo), URLs, and duplicate space\n",
    "uber_regex =  \"|\".join([user_regex, url_regex, hashtag_regex, space_regex])# , emoji_regex])\n",
    "\n",
    "data = data.withColumn(\"norm_text\", trim(strip_emo_udf(regexp_replace(\"text\", uber_regex, \"\"))))\n",
    "data = data.select(\"text\", \"norm_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import DocumentAssembler, Finisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the dataframe\n",
    "document_assembler = DocumentAssembler() \\\n",
    "            .setInputCol(\"norm_text\")\n",
    "    \n",
    "sentence_detector = SentenceDetectorModel() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = RegexTokenizer() \\\n",
    "            .setInputCols([\"sentence\"]) \\\n",
    "            .setOutputCol(\"token\")\n",
    "        \n",
    "normalizer = Normalizer() \\\n",
    "            .setInputCols([\"token\"]) \\\n",
    "            .setOutputCol(\"normal\")        \n",
    "        \n",
    "spell_checker = NorvigSweetingApproach() \\\n",
    "            .setInputCols([\"normal\"]) \\\n",
    "            .setOutputCol(\"spell\")\n",
    "        \n",
    "sentiment_detector = ViveknSentimentApproach() \\\n",
    "    .setInputCols([\"spell\", \"sentence\"]) \\\n",
    "    .setOutputCol(\"sentiment\") \\\n",
    "    .setPositiveSource(\"vivekn/positive\") \\\n",
    "    .setNegativeSource(\"vivekn/negative\") \\\n",
    "    .setPruneCorpus(False) # when training on small data you may want to disable this to not cut off infrequent words\n",
    "    \n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"sentiment\"]) \\\n",
    "    .setIncludeKeys(True) \\\n",
    "    ##.setCleanAnnotations(False)\n",
    "    \n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    spell_checker,\n",
    "    sentiment_detector,\n",
    "    finisher\n",
    "])\n",
    "\n",
    "sentiment_data = pipeline.fit(data).transform(data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def round_up(d):\n",
    "    return round(d + .01)\n",
    "\n",
    "def calc_mean(s):\n",
    "    ls = list(map(lambda x: 1 if (x == \"result->positive\") else 0, s.split(\"@\")))\n",
    "    return round_up(sum(ls) / len(ls))\n",
    "\n",
    "calc_mean_udf = udf(calc_mean, IntegerType())\n",
    "\n",
    "sentiment_data = sentiment_data.withColumn(\"mean_sentiment\", calc_mean_udf(\"finished_sentiment\"))\n",
    "sentiment_data.take(20)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
