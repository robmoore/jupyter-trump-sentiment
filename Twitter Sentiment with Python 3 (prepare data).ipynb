{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "This notebook was developed using the [all-spark-notebook](https://github.com/jupyter/docker-stacks/tree/master/all-spark-notebook) and should work without modification in that environment.\n",
    "\n",
    "Earlier this year, I produced a [similar notebook](https://www.zepl.com/viewer/notebooks/bm90ZTovL3JvYm9yYXRpdmUvVHdpdHRlci1TZW50aW1lbnQtRXhhbXBsZS9hNTlmZjFkYTAzY2Y0ZWY0YTg5MWRlNjZkMWFlM2I0My9ub3RlLmpzb24) using [Zeppelin](https://zeppelin.apache.org/). In that project, I used the [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/) library to analyze sentiment of a Twitter stream. That library is not specific to Spark but is written in Java so it's trivial to use it from Spark. Out of curiousity, I recently looked into the current NLP space in the Spark ecosystem to see if anything Spark-specific had been released in the interim and came across a recently released [NLP library](http://www.johnsnowlabs.com/dataops-blog/natural-language-processing-library/) from [John Snow Labs](http://www.johnsnowlabs.com). The library includes a sentiment analyzer which I hadn't heard of before which is based on [research](http://arxiv.org/abs/1305.6143) and [code](https://github.com/vivekn/sentiment) produced by Vivek Narayanan, Ishan Arora, and Arjun Bhatia. In the paper, the authors claim an 88% accuracy on the Internet Movie Database (IMDb).\n",
    "\n",
    "As I considered this project, I came across a [data set of tweets about Trump](https://www.kaggle.com/ahsanijaz/trumptweets) which includes 4600 tweets collected on August 17, 2015 and manually labeled as positive or negative. This data is tremedously helpful for evaluating a sentiment analyzer as the hard work of providing classified data has already been performed by the provider. \n",
    "\n",
    "In comparison to the IMDB data set of 25000 reviews, the tweet data set is much more limited in number and in length of material. Nevertheless, the analysis of the tweet data set resulted in an accuracy value of 91%. As the Vivekn et al approach is based on a Naive Bayes model which lends itself to shorter text according to [prior research](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf), it may explain the robust performance of the algorithm with this data.\n",
    "\n",
    "Before I could begin to analyze the data, I needed to do some data mangling. The source file (`trumptweets.csv`) appears to have a Windows-1252 encoding and has fewer columns than the data it describes. While the data is loaded using the Windows-1252 encoding, artifacts remain that could be influencing the analsysis of the tweets and may be worth analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trumptweet_mod.csv already exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -f trumptweet_mod.csv ]; then\n",
    "    echo \"Creating copy of trumptweet.csv with modifications\"\n",
    "    sed 's/\"X\",/\"X\",\"X_copy\",/' trumptweet.csv > trumptweet_mod.csv # Add new column to header to align header with data\n",
    "    sed -i 's/\"X.1\",/\"X_1\",/g' trumptweet_mod.csv # Spark reader behaves oddly when like dots are in column names\n",
    "    sed -i 's/\\\\\"/\"\"/g' trumptweet_mod.csv # Use standard quote escapes for CSV rather than backslashes\n",
    "else\n",
    "    echo \"trumptweet_mod.csv already exists\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "# Default settings resulted in GC issues. However, may be able to reduce this if system memory is limited\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "conf.set(\"spark.driver.memory\", \"4g\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf = conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|    0| 2262|\n",
      "|    1| 2340|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use modified version of the data\n",
    "labeled_data = spark.read.csv(\"trumptweet_mod.csv\", header = True, escape = '\"', encoding = \"windows-1252\", mode = \"FAILFAST\", multiLine = \"true\")\n",
    "\n",
    "# Drop possible NAs or null values for safety's sake\n",
    "labeled_data = labeled_data.filter((labeled_data.Class == '0') | (labeled_data.Class == '1'))\n",
    "\n",
    "# Show breakdown of positive and negative tweets\n",
    "labeled_data.groupBy('Class').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           norm_text|Class|\n",
      "+--------------------+-----+\n",
      "|Illegals must be ...|    1|\n",
      "|is there any othe...|    0|\n",
      "|Caring - The GOP ...|    0|\n",
      "|So much stupid go...|    0|\n",
      "|THE TRUMP IMMIGRA...|    1|\n",
      "|Christie on Donal...|    0|\n",
      "|Not a Trump fan, ...|    1|\n",
      "|Court Has To Step...|    0|\n",
      "|Trump is correct ...|    1|\n",
      "|\"I�m going to pre...|    1|\n",
      "|I really hope peo...|    0|\n",
      "|Trump is claiming...|    0|\n",
      "|Latest poll has T...|    1|\n",
      "|BOOM � Univision ...|    1|\n",
      "|I am now all in f...|    1|\n",
      "|\"His ratings amon...|    0|\n",
      "|Today's Trump pos...|    0|\n",
      "|Donald trump the ...|    0|\n",
      "|A little surprise...|    1|\n",
      "|Trump is the last...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize tweets\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import trim\n",
    "\n",
    "rt_regex = r\"(?=\\s?)(RT:?)(?=\\s?)\" # Drop RT\n",
    "user_regex = r\"@\\S+\" # Drop user references\n",
    "url_regex = r\"http[s]?:\\S+\" # Drop URLs\n",
    "hashtag_regex = r\"#\" # Remove hashtag itself but leave tag value\n",
    "unicode_regex = r\"<ed>|<U\\+[^>]*>\" # Clean up spurious unicode references\n",
    "space_etc_regex = r\"^[\\.'\\\",]\\s+|\\s{2,}|\\n\" # Remove extra spaces, linefeeds and leading periods or commas.\n",
    "\n",
    "# Remove RT, users (@foo), URLs, #s, line feeds, odd unicode 'tags'\n",
    "uber_regex =  \"|\".join([rt_regex, user_regex, url_regex, hashtag_regex, unicode_regex])\n",
    "\n",
    "labeled_data = labeled_data.withColumn(\"norm_text\", regexp_replace(\"text\", uber_regex, \"\"))\n",
    "labeled_data = labeled_data.withColumn(\"norm_text\", trim(regexp_replace(\"norm_text\", space_etc_regex, \" \")))\n",
    "labeled_data = labeled_data.filter(labeled_data.norm_text != '') # Drop any text that is now empty from actions above\n",
    "labeled_data = labeled_data.select(\"norm_text\", \"Class\") # Drop most of the data as isn't used\n",
    "labeled_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data 80%/20% for training and testing\n",
    "train_data, test_data = labeled_data.randomSplit([0.8, 0.2], seed=71082)\n",
    "\n",
    "# NLP code below requires separate files for both positive and negative training data\n",
    "# After much experimentation, not able to simple write to filesystem using Spark libraries\n",
    "# as it produces multiple files and metadata files that the Vivekn annotator cannot process.\n",
    "def write_df(df, dirname, filename):\n",
    "    # Use Hadoop library directly to write to filesystem\n",
    "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "    Configuration = sc._gateway.jvm.org.apache.hadoop.conf.Configuration\n",
    "\n",
    "    tmp_name = filename + \".tmp\"\n",
    "    # Write to a single file rather than multiple\n",
    "    df.coalesce(1).write.mode('overwrite').text(tmp_name)\n",
    "    \n",
    "    fs = FileSystem.get(Configuration())\n",
    "    fs.mkdirs(Path(dirname))\n",
    "    # Assume one file output\n",
    "    file = fs.globStatus(Path(tmp_name + \"/*.txt\"))[0].getPath();\n",
    "    fs.rename(file, Path(dirname + \"/\" + filename));\n",
    "    fs.delete(Path(tmp_name), True);\n",
    "       \n",
    "# split training data into positive/negative\n",
    "positive_data = train_data.filter(train_data.Class == \"1\").select(\"norm_text\")\n",
    "write_df(positive_data, \"train-data.txt\", \"positive.txt\")\n",
    "\n",
    "negative_data = labeled_data.filter(train_data.Class == \"0\").select(\"norm_text\")\n",
    "write_df(negative_data, \"train-data.txt\", \"negative.txt\")\n",
    "\n",
    "test_data.write.mode(\"overwrite\").parquet(\"test-data.parquet\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
