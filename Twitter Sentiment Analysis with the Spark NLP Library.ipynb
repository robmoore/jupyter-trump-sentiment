{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis with the Spark NLP Library\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This notebook was developed using the [all-spark-notebook](https://github.com/jupyter/docker-stacks/tree/master/all-spark-notebook) and should work without modification in that environment.\n",
    "\n",
    "Earlier this year, I produced a [similar notebook](https://www.zepl.com/viewer/notebooks/bm90ZTovL3JvYm9yYXRpdmUvVHdpdHRlci1TZW50aW1lbnQtRXhhbXBsZS9hNTlmZjFkYTAzY2Y0ZWY0YTg5MWRlNjZkMWFlM2I0My9ub3RlLmpzb24) using [Zeppelin](https://zeppelin.apache.org/). In that project, I used the [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/) library to analyze sentiment of a Twitter stream of tweets about Donald Trump. The CoreNLP library is not specific to Spark but is written in Java so it's trivial to use it from within Spark. However, recent investigation into the current NLP space in the Spark ecosystem unearthed the recently released [NLP library](http://nlp.johnsnowlabs.com/index.html) from [John Snow Labs](http://www.johnsnowlabs.com). The library includes a sentiment analyzer which utilizes a novel approach based on the [research](http://arxiv.org/abs/1305.6143) and [code](https://github.com/vivekn/sentiment) produced by Vivek Narayanan et al. In the paper, the authors claim an 88% accuracy on sentiment prediction of reviews from the Internet Movie Database (IMDb).\n",
    "\n",
    "As I considered this project, I came across a [data set of tweets about Trump](https://www.kaggle.com/ahsanijaz/trumptweets) which includes 4600 tweets collected on August 17, 2015. The data set stood out because the tweets were manually labeled as positive or negative. Of course, having pre-labeled data has proven tremedously helpful in evaluating a supervised sentiment analyzer as the labor-intensive work of providing classified data has already been performed by the provider (ahsanijaz). \n",
    "\n",
    "In comparison to the IMDB data set of 25000 reviews, the tweet data set is much more limited in number and in length of material. Nevertheless, in this notebook the analysis of the tweet data set resulted in an accuracy of 82.6%. As the Vivekn et al approach is based on a Naive Bayes model which lends itself to shorter text according to [prior research](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf), it appear to be a good choice for analyzing this data.\n",
    "\n",
    "One factor that made a significant difference in the performance of the model is the approach taken here of not removing duplicate data from the training set. If duplicate data is removed from the set the accuracy drops to 78%. Duplicate tweets result primarily from the processing approach taken below in which retweets are represented in the data. Furthermore, the common indicator of a retweet ('RT') has been removed from the tweets which results in a modest futher redundancy. It may be the case that the redundancy helps reinforce the general mood of the population and thereby helps shift the weighting in a beneficial way. However, it also seems likely that the approach may cause overfitting of the data as it is more probable that duplicate tweets will appear in both the training and test sets. In this notebook, I have not tested this hypothesis but it certainly would be beneficial in getting better confidence in the resulting model.\n",
    "\n",
    "### Preparation details\n",
    "\n",
    "Before I could begin to analyze the data, data wrangling was necessary. The source file (`trumptweets.csv`) appears to have a Windows-1252 encoding and oddly has fewer columns defined than the data it describes. While the data is loaded using the Windows-1252 encoding, encoding artifacts remain that could influence the analsysis of the tweets and may be worth further analysis.\n",
    "\n",
    "In order to align the data with the defined columns, I add a nonce header value. Furthermore, the Spark CSV parser behaves in a subtlely different way when periods (`.`) exist in the data so I removed these as well. Finally, the data provider used backslashes to escape quotes (`\\\"`) rather than the more common approach of using double quotes (`\"\"`) so I substituted the former with the later so that the Spark CSV parser would work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trumptweet_mod.csv already exists so leaving unmodified\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -f trumptweet_mod.csv ]; then\n",
    "    echo \"Creating copy of trumptweet.csv with modifications\"\n",
    "    iconv -f CP1252 -t UTF-8 trumptweet.csv > trumptweet_mod.csv # Converting to UTF-8\n",
    "    sed -i 's/\"X\",/\"X\",\"X_copy\",/' trumptweet_mod.csv # Add new column to header to align header with data\n",
    "    sed -i 's/\"X.1\",/\"X_1\",/' trumptweet_mod.csv # Spark reader behaves oddly when like dots are in column names\n",
    "    sed -i 's/\\\\\"/\"\"/g' trumptweet_mod.csv # Use standard quote escapes for CSV rather than backslashes\n",
    "else\n",
    "    echo \"trumptweet_mod.csv already exists so leaving unmodified\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by installing the Spark NLP library into the environment as it is not included in the Jupyter environment noted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.6/site-packages\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from ftfy)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Load Spark NLP package as it's not included in all-spark-notebook\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages JohnSnowLabs:spark-nlp:1.2.3 pyspark-shell'\n",
    "\n",
    "!pip install ftfy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my local machine, I encountered garbage collection issues processing the data using the default configuration. Increasing the memory for both the executor and the driver were successful in resolving these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "# Default settings resulted in GC issues. However, may be able to reduce this if system memory is limited\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "conf.set(\"spark.driver.memory\", \"4g\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf = conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains linefeeds so the `multiLine` parameter was required to process the entire data set. As is seen below, the data contains a roughly equal number of positive and negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|    0| 2262|\n",
      "|    1| 2340|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use modified version of the data\n",
    "labeled_data = spark.read.csv(\"trumptweet_mod.csv\", header = True, escape = '\"', mode = \"FAILFAST\", multiLine = \"true\")\n",
    "\n",
    "# Show breakdown of positive and negative tweets\n",
    "labeled_data.groupBy('Class').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I perform the following normalizations on the data :\n",
    "\n",
    "- Remove `RT`\n",
    "- Remove user references (eg, `@realDonaldTrump`)\n",
    "- Remove URLs\n",
    "- Remove the hashtag character (`#`)\n",
    "- Remove odd Unicode and unexplained 'tags' (eg, `<ed><U+00A0>`)\n",
    "- Removed stray leading characters (`.`, `'`, and `\"`)\n",
    "- Removed whitespace and beginning and end of tweet\n",
    "- Removed any entries that resulted in empty text after the above processing\n",
    "- Removed all columns except for the normalized text and the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           norm_text|Class|\n",
      "+--------------------+-----+\n",
      "|@GOPBlackChick: I...|    1|\n",
      "|@CNN is there any...|    0|\n",
      "|@KurtSchlichter: ...|    0|\n",
      "|@ajpeacemaker @md...|    0|\n",
      "|THE TRUMP IMMIGRA...|    1|\n",
      "|@CNNPolitics: Chr...|    0|\n",
      "|@Morning_Joe Not ...|    1|\n",
      "|@ThePatriot143: C...|    0|\n",
      "|Trump is correct ...|    1|\n",
      "|\"I'm going to pre...|    1|\n",
      "|I really hope peo...|    0|\n",
      "|Trump is claiming...|    0|\n",
      "|@marclamonthill: ...|    1|\n",
      "|BOOM â€“ Univision ...|    1|\n",
      "|@GeoScarborough I...|    1|\n",
      "|@AlexBlackStars @...|    0|\n",
      "|@charlescwcooke: ...|    0|\n",
      "|@mdabbss: Donald ...|    0|\n",
      "|A little surprise...|    1|\n",
      "|@Salon: Trump is ...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize tweets\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import trim\n",
    "from pyspark.sql.functions import udf\n",
    "from ftfy import fix_text\n",
    "\n",
    "fix_text_udf = udf(lambda s: fix_text(s))\n",
    "\n",
    "rt_regex = r\"(?=\\s?)(RT:?)(?=\\s?)\" # Drop RT\n",
    "user_regex = r\"@\\S+\" # Drop user references\n",
    "url_regex = r\"http[s]?:\\S+\" # Drop URLs\n",
    "hashtag_regex = r\"#\" # Remove hashtag itself but leave tag value\n",
    "unicode_regex = r\"<ed>|<U\\+[^>]*>\" # Clean up spurious unicode references\n",
    "space_etc_regex = r\"^[\\.'\\\",]\\s+|\\s{2,}|\\n\" # Remove extra spaces, linefeeds and leading periods or commas.\n",
    "\n",
    "# Remove RT, users (@foo), URLs, #s, line feeds, odd unicode 'tags'\n",
    "#uber_regex =  \"|\".join([rt_regex, user_regex, url_regex, hashtag_regex, unicode_regex])\n",
    "uber_regex =  \"|\".join([rt_regex, url_regex, hashtag_regex, unicode_regex])\n",
    "\n",
    "labeled_data = labeled_data.withColumn(\"norm_text\", fix_text_udf(\"text\"))\n",
    "labeled_data = labeled_data.withColumn(\"norm_text\", regexp_replace(\"norm_text\", uber_regex, \"\"))\n",
    "labeled_data = labeled_data.withColumn(\"norm_text\", trim(regexp_replace(\"norm_text\", space_etc_regex, \" \")))\n",
    "labeled_data = labeled_data.filter(labeled_data.norm_text != '') # Drop any text that is now empty from actions above\n",
    "labeled_data = labeled_data.select(\"norm_text\", \"Class\") # Drop most of the data as isn't used\n",
    "labeled_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vivekn annotator requires separate files for both positive and negative training data. While there is not much documentation on the expected input (that is, whether it will accept a Spark directory containing multiple files as is common with Hadoop files), my attempts to utilize such an approach were not successful. As a result, I resorted to a hybrid approach of writing out the training data using the standard Spark API and then modifying the resulting file by accessing the underlying Hadoop libraries directly. I believe there is likely a better approach to doing so but I leave it as an exercise to the reader to discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data 80%/20% for training and testing\n",
    "train_data, test_data = labeled_data.randomSplit([0.8, 0.2], seed=71082)\n",
    "\n",
    "# NLP code below requires separate files for both positive and negative training data.\n",
    "# After much experimentation, not able to simple write to filesystem using Spark libraries\n",
    "# as it produces both the data file and metadata files that the Vivekn annotator cannot process\n",
    "# when simply indicating the directory where the file exists in the code below.\n",
    "def write_df(df, dirname, filename):\n",
    "    # Use Hadoop library directly to write to filesystem\n",
    "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "    Configuration = sc._gateway.jvm.org.apache.hadoop.conf.Configuration\n",
    "\n",
    "    tmp_name = filename + \".tmp\"\n",
    "    # Write to a single file rather than multiple\n",
    "    df.coalesce(1).write.mode('overwrite').text(tmp_name)\n",
    "    \n",
    "    fs = FileSystem.get(Configuration())\n",
    "    fs.mkdirs(Path(dirname))\n",
    "    # Assume one file output\n",
    "    file = fs.globStatus(Path(tmp_name + \"/*.txt\"))[0].getPath();\n",
    "    fs.rename(file, Path(dirname + \"/\" + filename));\n",
    "    fs.delete(Path(tmp_name), True);\n",
    "    \n",
    "# split training data into positive/negative\n",
    "positive_data = train_data.filter(train_data.Class == \"1\").select(\"norm_text\")\n",
    "write_df(positive_data, \"train-data.txt\", \"positive.txt\")\n",
    "\n",
    "negative_data = labeled_data.filter(train_data.Class == \"0\").select(\"norm_text\")\n",
    "# remove duplicates\n",
    "negative_data = negative_data.distinct()\n",
    "write_df(negative_data, \"train-data.txt\", \"negative.txt\")\n",
    "\n",
    "test_data.write.mode(\"overwrite\").parquet(\"test-data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment prediction\n",
    "\n",
    "Now that we have the training and test data ready to go, it's time to put it to use!\n",
    "\n",
    "After reading in the test data, we remove duplicate entries so as not to bias the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           norm_text|Class|\n",
      "+--------------------+-----+\n",
      "|\"@WIRED: There's ...|    1|\n",
      "|\"Disgusting, bigo...|    0|\n",
      "|\"Former Reagan An...|    0|\n",
      "|\"I'm going to pre...|    1|\n",
      "|'Dilbert' creator...|    0|\n",
      "|'I don't care if ...|    1|\n",
      "|.@KatiePavlich sa...|    0|\n",
      "|0/ da heck he sai...|    0|\n",
      "|15 Things Trump a...|    1|\n",
      "|20 times Donald T...|    0|\n",
      "|5 Marketing Lesso...|    1|\n",
      "|@97Musick: @FoxNe...|    1|\n",
      "|@ABCPolitics: Don...|    1|\n",
      "|@ABCPolitics: Don...|    1|\n",
      "|@AG_Conservative:...|    0|\n",
      "|@AdamofAlbion: If...|    0|\n",
      "|@AdamsFlaFan: Don...|    0|\n",
      "|@AlexAtFWW: Wait,...|    0|\n",
      "|@AllenWest: Trump...|    1|\n",
      "|@AnitaPaoPao: @_A...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data produced earlier\n",
    "test_data = spark.read.parquet(\"test-data.parquet\")\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now execute the NLP pipeline using the Vivekn annotator to create a model and make sentiment predictions on our test tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|           norm_text|Class|  finished_sentiment|\n",
      "+--------------------+-----+--------------------+\n",
      "|\"@WIRED: There's ...|    1|    result->positive|\n",
      "|\"Disgusting, bigo...|    0|    result->negative|\n",
      "|\"Former Reagan An...|    0|    result->negative|\n",
      "|\"I'm going to pre...|    1|result->positive@...|\n",
      "|'Dilbert' creator...|    0|result->positive@...|\n",
      "|'I don't care if ...|    1|    result->negative|\n",
      "|.@KatiePavlich sa...|    0|result->negative@...|\n",
      "|0/ da heck he sai...|    0|result->negative@...|\n",
      "|15 Things Trump a...|    1|    result->negative|\n",
      "|20 times Donald T...|    0|    result->negative|\n",
      "|5 Marketing Lesso...|    1|result->negative@...|\n",
      "|@97Musick: @FoxNe...|    1|result->positive@...|\n",
      "|@ABCPolitics: Don...|    1|    result->positive|\n",
      "|@ABCPolitics: Don...|    1|    result->positive|\n",
      "|@AG_Conservative:...|    0|    result->negative|\n",
      "|@AdamofAlbion: If...|    0|    result->negative|\n",
      "|@AdamsFlaFan: Don...|    0|    result->negative|\n",
      "|@AlexAtFWW: Wait,...|    0|    result->negative|\n",
      "|@AllenWest: Trump...|    1|result->positive@...|\n",
      "|@AnitaPaoPao: @_A...|    0|    result->negative|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "\n",
    "document_assembler = DocumentAssembler().setInputCol(\"norm_text\")\n",
    "    \n",
    "sentence_detector = SentenceDetectorModel().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = RegexTokenizer().setInputCols([\"sentence\"]).setOutputCol(\"token\")\n",
    "        \n",
    "normalizer = Normalizer().setInputCols([\"token\"]).setOutputCol(\"normal\")        \n",
    "        \n",
    "spell_checker = NorvigSweetingApproach().setInputCols([\"normal\"]).setOutputCol(\"spell\")\n",
    "        \n",
    "sentiment_detector = ViveknSentimentApproach().setInputCols([\"spell\", \"sentence\"]) \\\n",
    "    .setOutputCol(\"sentiment\").setPositiveSource(\"train-data.txt/positive.txt\") \\\n",
    "    .setNegativeSource(\"train-data.txt/negative.txt\").setPruneCorpus(False)   \n",
    "    \n",
    "finisher = Finisher().setInputCols([\"sentiment\"]).setIncludeKeys(True)\n",
    "    \n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    spell_checker,\n",
    "    sentiment_detector,\n",
    "    finisher\n",
    "])\n",
    "\n",
    "sentiment_data = pipeline.fit(test_data).transform(test_data)    \n",
    "sentiment_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline adds an extra column with an sentiment indicated for each sentence in the tweet. The value of the indication is either `result->positive` or `result-negative` and multiple sentences are represented with the repetition of these values delimited by `@`.  In order to get a single Boolean value for the tweet, we take the average of each sentiment indication and then round the result to 0 or 1. In Python 3, the result of rounding a value of .5 is 0. Arbitrarily, I decided to be biased towards positive sentiments by adding a small value to force .5 to round to 1. In testing, I found coincidentally that doing so resulted in more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+---------------+\n",
      "|           norm_text|Class|  finished_sentiment|total_sentiment|\n",
      "+--------------------+-----+--------------------+---------------+\n",
      "|\"@WIRED: There's ...|    1|    result->positive|              1|\n",
      "|\"Disgusting, bigo...|    0|    result->negative|              0|\n",
      "|\"Former Reagan An...|    0|    result->negative|              0|\n",
      "|\"I'm going to pre...|    1|result->positive@...|              1|\n",
      "|'Dilbert' creator...|    0|result->positive@...|              1|\n",
      "|'I don't care if ...|    1|    result->negative|              0|\n",
      "|.@KatiePavlich sa...|    0|result->negative@...|              0|\n",
      "|0/ da heck he sai...|    0|result->negative@...|              0|\n",
      "|15 Things Trump a...|    1|    result->negative|              0|\n",
      "|20 times Donald T...|    0|    result->negative|              0|\n",
      "|5 Marketing Lesso...|    1|result->negative@...|              1|\n",
      "|@97Musick: @FoxNe...|    1|result->positive@...|              1|\n",
      "|@ABCPolitics: Don...|    1|    result->positive|              1|\n",
      "|@ABCPolitics: Don...|    1|    result->positive|              1|\n",
      "|@AG_Conservative:...|    0|    result->negative|              0|\n",
      "|@AdamofAlbion: If...|    0|    result->negative|              0|\n",
      "|@AdamsFlaFan: Don...|    0|    result->negative|              0|\n",
      "|@AlexAtFWW: Wait,...|    0|    result->negative|              0|\n",
      "|@AllenWest: Trump...|    1|result->positive@...|              1|\n",
      "|@AnitaPaoPao: @_A...|    0|    result->negative|              0|\n",
      "+--------------------+-----+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from statistics import mean\n",
    "\n",
    "def sigmoid(s):\n",
    "    return 0 if s is None else round(mean(map(lambda x: 1 if (x == \"result->positive\") else 0, s.split(\"@\"))) + .01)\n",
    "\n",
    "sigmoid_udf = udf(sigmoid, IntegerType())\n",
    "\n",
    "sentiment_data = sentiment_data.withColumn(\"total_sentiment\", sigmoid_udf(\"finished_sentiment\"))\n",
    "sentiment_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we determine the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 729 correct classifications out of 869 observations: 83.89%\n"
     ]
    }
   ],
   "source": [
    "correct_count = sentiment_data.filter(sentiment_data.Class == sentiment_data.total_sentiment).count()\n",
    "total_count =  sentiment_data.count()\n",
    "accuracy = correct_count / total_count\n",
    "\n",
    "print(f\"Total of {correct_count} correct classifications out of {total_count} observations: {accuracy:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
