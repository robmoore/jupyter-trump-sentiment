{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages JohnSnowLabs:spark-nlp:1.2.3 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "#conf.set(\"spark.executor.memory\", \"8g\")\n",
    "\n",
    "# create the context\n",
    "sc = pyspark.SparkContext(conf = conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = spark.read.parquet(\"test-data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|           norm_text|Class|  finished_sentiment|\n",
      "+--------------------+-----+--------------------+\n",
      "|\"Yes, Donald Trum...|    0|result->negative@...|\n",
      "|. Donald Trump is...|    1|    result->positive|\n",
      "|. Yes. We learned...|    1|result->positive@...|\n",
      "|. reports for New...|    0|    result->positive|\n",
      "|Actually, Harbaug...|    0|    result->negative|\n",
      "|After your videos...|    1|    result->negative|\n",
      "|At a sad loss whe...|    0|    result->negative|\n",
      "|Bernie Sanders bl...|    0|    result->negative|\n",
      "|Bush with subtle ...|    0|    result->negative|\n",
      "|Byron York's rece...|    1|    result->positive|\n",
      "|Conservative Expe...|    0|    result->negative|\n",
      "|Donald Trump Does...|    1|    result->positive|\n",
      "|Donald Trump arri...|    1|    result->positive|\n",
      "|Donald Trump can ...|    0|    result->negative|\n",
      "|Donald Trump says...|    0|    result->negative|\n",
      "|Exceptionally stu...|    0|    result->negative|\n",
      "|FOX moderators fo...|    1|    result->negative|\n",
      "|Full Interview: C...|    0|result->negative@...|\n",
      "|Glenn Beck viciou...|    0|    result->negative|\n",
      "|Hell no 18 millio...|    0|    result->negative|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "\n",
    "document_assembler = DocumentAssembler().setInputCol(\"norm_text\")\n",
    "    \n",
    "sentence_detector = SentenceDetectorModel().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = RegexTokenizer().setInputCols([\"sentence\"]).setOutputCol(\"token\")\n",
    "        \n",
    "normalizer = Normalizer().setInputCols([\"token\"]).setOutputCol(\"normal\")        \n",
    "        \n",
    "spell_checker = NorvigSweetingApproach().setInputCols([\"normal\"]).setOutputCol(\"spell\")\n",
    "        \n",
    "# sentiment_detector = ViveknSentimentApproach().setInputCols([\"spell\", \"sentence\"]) \\\n",
    "#     .setOutputCol(\"sentiment\").setPositiveSource(\"trumptweet/positive/1.txt\") \\\n",
    "#     .setNegativeSource(\"trumptweet/negative/1.txt\").setPruneCorpus(False)\n",
    "\n",
    "sentiment_detector = ViveknSentimentApproach().setInputCols([\"spell\", \"sentence\"]) \\\n",
    "    .setOutputCol(\"sentiment\").setPositiveSource(\"trumptweet/positive.txt\") \\\n",
    "    .setNegativeSource(\"trumptweet/negative.txt\").setPruneCorpus(False)   \n",
    "    \n",
    "finisher = Finisher().setInputCols([\"sentiment\"]).setIncludeKeys(True)\n",
    "    \n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    spell_checker,\n",
    "    sentiment_detector,\n",
    "    finisher\n",
    "])\n",
    "\n",
    "sentiment_data = pipeline.fit(test_data).transform(test_data)    \n",
    "sentiment_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaObject.__init__.<locals>.<lambda> at 0x7feffd1bfd90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1169, in <lambda>\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 555, in _garbage_collect_object\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1028, in send_command\n",
      "  File \"/opt/conda/lib/python3.6/socket.py\", line 586, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <function JavaObject.__init__.<locals>.<lambda> at 0x7feffd1bfe18>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1169, in <lambda>\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 555, in _garbage_collect_object\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1028, in send_command\n",
      "  File \"/opt/conda/lib/python3.6/socket.py\", line 586, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <function JavaObject.__init__.<locals>.<lambda> at 0x7feffd1bfd08>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1169, in <lambda>\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 555, in _garbage_collect_object\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1028, in send_command\n",
      "  File \"/opt/conda/lib/python3.6/socket.py\", line 586, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def sigmoid(s):\n",
    "    if s is not None:\n",
    "        ls = list(map(lambda x: 1 if (x == \"result->positive\") else 0, s.split(\"@\")))\n",
    "        return 0 if (len(ls) == 0) else round(sum(ls) / len(ls) + .01)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "sigmoid_udf = udf(sigmoid, IntegerType())\n",
    "\n",
    "sentiment_data = sentiment_data.withColumn(\"total_sentiment\", sigmoid_udf(\"finished_sentiment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count = sentiment_data.filter(sentiment_data.Class == sentiment_data.total_sentiment).count()\n",
    "total_count =  sentiment_data.count()\n",
    "correct_count / total_count"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
